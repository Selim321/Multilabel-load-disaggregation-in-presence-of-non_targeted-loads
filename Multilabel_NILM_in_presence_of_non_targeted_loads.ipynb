{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multilabel NILM in presence of non-targeted loads",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1BPru2PUfJv8nhF2K0PYdqJlbo4E6Lrq9",
      "authorship_tag": "ABX9TyNvM95CtUaGa/wcbxCa7a+G"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCzgI4Av_2OP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69f58c52-ad0c-4b99-c4c5-c04d035abc96"
      },
      "source": [
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "from scipy import stats\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.multioutput import ClassifierChain \n",
        "from sklearn.metrics import f1_score, jaccard_score\n",
        "\n",
        "\n",
        "!pip install scikit-multilearn\n",
        "\n",
        "from skmultilearn.cluster import MatrixLabelSpaceClusterer\n",
        "from sklearn.cluster import KMeans\n",
        "from skmultilearn.ensemble import  LabelSpacePartitioningClassifier\n",
        "from skmultilearn.problem_transform import LabelPowerset\n",
        "from sklearn.naive_bayes import GaussianNB"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scikit-multilearn\n",
            "  Downloading scikit_multilearn-0.2.0-py3-none-any.whl (89 kB)\n",
            "\u001b[K     |████████████████████████████████| 89 kB 3.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: scikit-multilearn\n",
            "Successfully installed scikit-multilearn-0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UU8gAp-TIDgv"
      },
      "source": [
        "os.getcwd()\n",
        "os.chdir('/content/drive/MyDrive/Colab Notebooks')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKmNERTpAUBt"
      },
      "source": [
        "from house1_data import *\n",
        "# from house2_data import *\n",
        "# from house3_data import *\n",
        "# from house4_data import *\n",
        "# from house5_data import *\n",
        "# from house6_data import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import lists:"
      ],
      "metadata": {
        "id": "0UGsTNErOlAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "appliancesData = redd_house1_appliances\n",
        "appliancesList = redd_house1_names\n",
        "targetAppsIndices = redd_house1_targetAppsIndices"
      ],
      "metadata": {
        "id": "4UmqEKDyO1HC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a bunch of functions:"
      ],
      "metadata": {
        "id": "5DZ-i2Y8PDTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" MEDIAN FILTER \"\"\"     \n",
        "def median(aggregate_vector):\n",
        "    window = 50\n",
        "    s = pd.Series(aggregate_vector)\n",
        "    median_df = s.rolling(window).median()\n",
        "    return median_df.fillna(0)\n",
        "\n",
        "def plotApp():\n",
        "    for i in range(0,len(appliancesData)):\n",
        "        print(\"{} : {}\\n\".format(appliancesList[i], i))\n",
        "    choice = int(input(\"choose number\"))\n",
        "    plt.plot(appliancesData[choice])\n",
        "    \n",
        "def plot_energy_break(appliancesData, appliancesList):\n",
        "    \"\"\" Create a matrix containing the data of each appliance \"\"\"\n",
        "    appliances_mtrx=np.zeros((len(appliancesData[0]),\n",
        "                              len(appliancesData)))\n",
        "    for i in range(0,len(appliancesData)):\n",
        "        appliances_mtrx[:,i]=appliancesData[i]\n",
        "    energy_vector = np.zeros((1,len(appliancesData))) \n",
        "    energy_vector = np.sum(appliances_mtrx, axis=0)\n",
        "    x = np.arange(len(appliancesData))\n",
        "    plt.bar(x, energy_vector)  \n",
        "    plt.xticks(x, appliancesList)\n",
        "\n",
        "def label_load (appliance_serie):\n",
        "    thr = 50\n",
        "    fr2 = appliance_serie\n",
        "    fr3 = fr2.copy()\n",
        "    fr3 = fr3.to_numpy()\n",
        "    fr3[fr3 < thr] = 0\n",
        "    fr3[fr3 >= thr] = 1\n",
        "    label_vector = fr3\n",
        "    return label_vector\n",
        "\n",
        "def label_load_adaptive (appliance_serie):\n",
        "    thr = float(stats.mode(appliance_serie.to_numpy())[0])+5\n",
        "    fr2 = appliance_serie\n",
        "    fr3 = fr2.copy()\n",
        "    fr3 = fr3.to_numpy()\n",
        "    fr3[fr3 < thr] = 0\n",
        "    fr3[fr3 >= thr] = 1\n",
        "    label_vector = fr3\n",
        "    return label_vector\n",
        "\n",
        "def agg_vector(appliancesData):\n",
        "    \"\"\" Create a matrix containing the data of each appliance \"\"\"\n",
        "    appliances_mtrx=np.zeros((len(appliancesData[0]),\n",
        "                              len(appliancesData)))\n",
        "    for i in range(0,len(appliancesData)):\n",
        "        appliances_mtrx[:,i]=appliancesData[i]\n",
        "        \n",
        "    aggregate_vector = np.zeros((appliances_mtrx.shape[0],1))\n",
        "    aggregate_vector = np.sum(appliances_mtrx, axis=1) \n",
        "    return aggregate_vector \n",
        "\n",
        "def agg_vector_only_targets(appliancesData, targetAppsIndices):\n",
        "    \"\"\" Create a matrix containing the data of each appliance \"\"\"\n",
        "    appliances_mtrx=np.zeros((len(appliancesData[0]),\n",
        "                              len(targetAppsIndices)))\n",
        "    for i in range(0,len(targetAppsIndices)):\n",
        "        appliances_mtrx[:,i]=median(appliancesData[ targetAppsIndices[i] ]).to_numpy()\n",
        "        \n",
        "    aggregate_vector = np.zeros((appliances_mtrx.shape[0],1))\n",
        "    aggregate_vector = np.sum(appliances_mtrx, axis=1) \n",
        "    return aggregate_vector\n",
        "\n",
        "def agg_vector_median(appliancesData):\n",
        "    \"\"\" Create a matrix containing the data of each appliance \"\"\"\n",
        "    appliances_mtrx=np.zeros((len(appliancesData[0]),\n",
        "                              len(appliancesData)))\n",
        "    for i in range(0,len(appliancesData)):\n",
        "        appliances_mtrx[:,i]=median(appliancesData[i]).to_numpy()\n",
        "        \n",
        "    aggregate_vector = np.zeros((appliances_mtrx.shape[0],1))\n",
        "    aggregate_vector = np.sum(appliances_mtrx, axis=1) \n",
        "    return aggregate_vector \n",
        "\n",
        "def checkLabeling():    \n",
        "    print(\"choose load:\")\n",
        "    for i in range(0,len(targetAppsIndices)):\n",
        "        print(\"{} : {}\\n\".format(appliancesList[targetAppsIndices[i]], targetAppsIndices[i]))\n",
        "    choice = int(input(\"choose number\"))\n",
        "    plt.plot(median(appliancesData[choice]))\n",
        "    column_index = np.where(targetAppsIndices_arr==choice)[0]\n",
        "    plt.plot(label_mtrx[:,column_index]*100)\n",
        "    plt.show()\n",
        "\n",
        "\"\"\" Dicretization: transform the dataset with KBinsDiscretizer \"\"\" \n",
        "def discret(aggregate_vector): \n",
        "    enc = KBinsDiscretizer(n_bins=100, encode='ordinal', strategy='uniform')\n",
        "    X_binned = enc.fit_transform(aggregate_vector.reshape(-1,1))\n",
        "    \n",
        "    aggregate_vector_denoised = enc.inverse_transform(X_binned)\n",
        "    return aggregate_vector_denoised\n",
        "\n",
        "\"\"\" Compute absolute changes \"\"\" \n",
        "def changes(aggregate_vector_denoised):\n",
        "    aggregate_vector_denoised_x = np.insert(aggregate_vector_denoised,0,0) \n",
        "    aggregate_vector_denoised_x_plus = np.insert(aggregate_vector_denoised,\n",
        "                                            len(aggregate_vector_denoised),0)\n",
        "    delta_x_vector = aggregate_vector_denoised_x_plus - aggregate_vector_denoised_x\n",
        "    return delta_x_vector\n",
        "\n",
        "\"\"\" Remove spikes from denoised signal \"\"\" \n",
        "def remove_spikes (noised_vector, delta_x):\n",
        "    aggregate_vector_denoised_corrected = np.zeros((len(noised_vector), 1))\n",
        "    delta_x_vector_corrected = np.zeros((len(delta_x), 1))\n",
        "    for i in range (1, len(noised_vector)):\n",
        "        if(delta_x[i]!=0):\n",
        "            if(delta_x[i+1]==0):\n",
        "                delta_x_vector_corrected[i] = delta_x[i]\n",
        "                aggregate_vector_denoised_corrected[i] = noised_vector[i]\n",
        "            else:\n",
        "                delta_x_vector_corrected[i] = delta_x[i-1]\n",
        "                delta_x_vector_corrected[i+1] = delta_x[i-1]\n",
        "                aggregate_vector_denoised_corrected[i] = noised_vector[i-1]\n",
        "        if(delta_x[i]==0):\n",
        "            delta_x_vector_corrected[i] = delta_x[i]\n",
        "            aggregate_vector_denoised_corrected[i] = noised_vector[i]\n",
        "    return aggregate_vector_denoised_corrected, delta_x_vector_corrected\n",
        "\n",
        "\"\"\" ************************* remove short events \"\"\"  \n",
        "def rmv_evnts(agrregate_final1, delta_x_final1):\n",
        "    duration_thresold = 5\n",
        "    events_index = np.where(delta_x_final1!=0)[0]\n",
        "    \n",
        "    \"\"\" find short events indices \"\"\" \n",
        "    short_events_index= []\n",
        "    for i in range(0, len(events_index)-1):\n",
        "        if(events_index[i+1]-events_index[i]<=duration_thresold):\n",
        "            duration = events_index[i+1]-events_index[i]\n",
        "            short_events_index.append((events_index[i],duration))\n",
        "    \n",
        "    \"\"\" remove the events \"\"\" \n",
        "    aggregate_final = np.copy(agrregate_final1)\n",
        "    delta_x_final = np.copy(delta_x_final1)\n",
        "    for i in range(0, len(short_events_index)-1):\n",
        "        delta_x_final[short_events_index[i][0]] = 0\n",
        "        # delta_x_final[short_events_index[i+1]] = 0\n",
        "        aggregate_final[short_events_index[i][0]:short_events_index[i][0]+short_events_index[i][1]] = aggregate_final[short_events_index[i][0]-1]\n",
        "      \n",
        "    \"\"\" new delta_x \"\"\" \n",
        "    aggregate_final_x = np.insert(aggregate_final,0,0) \n",
        "    aggregate_final_x_plus = np.insert(aggregate_final,\n",
        "                                            len(aggregate_final),0)\n",
        "    delta_x_final = aggregate_final_x_plus - aggregate_final_x  \n",
        "    \n",
        "    return aggregate_final, delta_x_final\n",
        "\n",
        "\"\"\" ******************* extract (delta_x, aggregate, duration) \"\"\"\n",
        "def features (aggregate_final, delta_x_final, label_mtrx):\n",
        "    events_indices = np.where(delta_x_final!=0)[0]\n",
        "    delta_p = []\n",
        "    state_p =[]\n",
        "    state_duration = []\n",
        "    label =[]\n",
        "    \n",
        "    for i in range(0, len(events_indices)-1):\n",
        "        delta_p.append(delta_x_final[events_indices[i]])\n",
        "        state_p.append(aggregate_final[events_indices[i]])\n",
        "        state_duration.append(events_indices[i+1] - events_indices[i])\n",
        "        label.append(label_mtrx[events_indices[i],:])\n",
        "        \n",
        "    delta_p = np.array(delta_p)\n",
        "    delta_p = delta_p.reshape(len(delta_p),1)   \n",
        "    state_p = np.array(state_p)\n",
        "    state_p = state_p.reshape(len(state_p),1)   \n",
        "    state_duration = np.array(state_duration)\n",
        "    state_duration = state_duration.reshape(len(state_duration),1) \n",
        "    \"\"\" don't forget \"\"\" \n",
        "    X = np.concatenate((delta_p,state_p,state_duration), axis=1) \n",
        "    # X = np.concatenate((delta_p,state_duration), axis=1)\n",
        "    Y = np.array(label)\n",
        "    \n",
        "    return X, Y\n",
        "\n",
        "def train_test (classifier, X_train, X_test):\n",
        "    if (classifier=='ovr'):\n",
        "        ovr_rf = OneVsRestClassifier(RandomForestClassifier(random_state=0)).fit(\n",
        "    X_train, Y_train)\n",
        "        Y_pred_train = ovr_rf.predict_proba(X_train)\n",
        "        Y_pred_test = ovr_rf.predict_proba(X_test)\n",
        "        \n",
        "    if (classifier=='chains'):\n",
        "        chains = [ClassifierChain(RandomForestClassifier(random_state=0), order='random', random_state=i)\n",
        "          for i in range(10)]\n",
        "        for chain in chains:\n",
        "            chain.fit(X_train, Y_train)\n",
        "        \n",
        "        Y_pred_chains = np.array([chain.predict(X_train) for chain in\n",
        "                          chains])\n",
        "        Y_pred_ensemble = Y_pred_chains.mean(axis=0)\n",
        "        Y_pred_train = Y_pred_ensemble \n",
        "        \n",
        "        Y_pred_chains = np.array([chain.predict(X_test) for chain in\n",
        "                  chains])\n",
        "        Y_pred_ensemble = Y_pred_chains.mean(axis=0)\n",
        "        Y_pred_test = Y_pred_ensemble \n",
        "        \n",
        "    \n",
        "    if (classifier=='mlknn'):\n",
        "        from skmultilearn.adapt import MLkNN\n",
        "\n",
        "        classifier = MLkNN(k=71, s=0.7)\n",
        "        \n",
        "        # train\n",
        "        classifier.fit(X_train, Y_train)\n",
        "        \n",
        "        # predict\n",
        "        Y_pred_train = classifier.predict(X_train)\n",
        "        Y_pred_test = classifier.predict(X_test)\n",
        "        \n",
        "    if (classifier=='lsp'):\n",
        "        \n",
        "        matrix_clusterer = MatrixLabelSpaceClusterer(clusterer=KMeans(n_clusters=2))\n",
        "        \n",
        "        matrix_clusterer.fit_predict(X_train, Y_train)\n",
        "        \n",
        "        classifier = LabelSpacePartitioningClassifier(\n",
        "            classifier = LabelPowerset(classifier=GaussianNB()),\n",
        "            clusterer = matrix_clusterer\n",
        "        )\n",
        "        \n",
        "        classifier.fit(X_train, Y_train)\n",
        "                \n",
        "        # predict\n",
        "        Y_pred_train = classifier.predict(X_train)\n",
        "        Y_pred_test = classifier.predict(X_test)\n",
        "                \n",
        "    return Y_pred_train, Y_pred_test\n"
      ],
      "metadata": {
        "id": "veh1GAkzPGAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some data manipulation:"
      ],
      "metadata": {
        "id": "1W_ipN4SPfev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "appliancesList_arr = np.array(appliancesList)\n",
        "targetAppsNames = appliancesList_arr[targetAppsIndices] \n",
        "targetAppsNames_arr = targetAppsNames.reshape((1,len(targetAppsNames)))\n",
        "targetAppsIndices_arr = np.array(targetAppsIndices)\n",
        "allIndices= np.arange(0,len(appliancesList))\n",
        "outlierAppsIndices = np.setdiff1d(allIndices, targetAppsIndices)\n",
        "outliersNames =   np.array(appliancesList)[outlierAppsIndices]\n",
        "\n",
        "number_of_targets = len(targetAppsIndices)\n",
        "print(\"number of target loads: {}\".format(number_of_targets))\n",
        "number_of_outliers = len(appliancesList) - number_of_targets \n",
        "print(\"number of non-targeted loads: {}\".format(number_of_outliers))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDckrXdePiFZ",
        "outputId": "dc6cce6e-2ba3-4152-da5c-989a53a4cf1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of target loads: 7\n",
            "number of non-targeted loads: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the labels matrix:"
      ],
      "metadata": {
        "id": "nnHZ45IxP_Vv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_mtrx= np.zeros((len(appliancesData[0]), (len(targetAppsIndices_arr))))\n",
        "\n",
        "for i in range(0, len(targetAppsIndices_arr)):\n",
        "    singlaAppData= appliancesData[ targetAppsIndices_arr[i] ]\n",
        "    # singlaAppData_arr = singlaAppData.to_numpy()\n",
        "    labelVector = label_load_adaptive(median(singlaAppData))\n",
        "    # labelVector_column = labelVector.reshape((1,len(labelVector)))\n",
        "    label_mtrx[:,i] = labelVector"
      ],
      "metadata": {
        "id": "O9JvnHR2QBwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the aggregate vector:"
      ],
      "metadata": {
        "id": "YGwQxTwXQoyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agg_vector_raw = agg_vector(appliancesData)\n",
        "    \n",
        "# agg_vector = agg_vector_median(appliancesData)\n",
        "agg_vector = agg_vector_only_targets(appliancesData,targetAppsIndices)"
      ],
      "metadata": {
        "id": "hLAogCPjQrpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discretization of the aggregate vector:"
      ],
      "metadata": {
        "id": "KIzPRYESRWXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agg_vector_disc = discret(agg_vector)"
      ],
      "metadata": {
        "id": "_GKXw3CMRanq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute absolute changes of the aggregate signal:"
      ],
      "metadata": {
        "id": "foEYOWubSJ3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delta_x_vector = changes(agg_vector_disc)"
      ],
      "metadata": {
        "id": "Lhv94ElwSOkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove spikes from the aggregate signal:"
      ],
      "metadata": {
        "id": "6CP845WySanu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agg_vector_filt_1, delta_x_vector_corrected_1 = remove_spikes(agg_vector_disc, delta_x_vector)"
      ],
      "metadata": {
        "id": "5Yq22E_LSd1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove short events from the aggregate signal:"
      ],
      "metadata": {
        "id": "V3SSKCBPStbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agg_vector_filt, delta_x_vector_corrected  = rmv_evnts(agg_vector_filt_1,\n",
        "                                                       delta_x_vector_corrected_1)"
      ],
      "metadata": {
        "id": "Jl3Y35jwSyy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature extraction:"
      ],
      "metadata": {
        "id": "ksigu_B5TD0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y = features (agg_vector_filt, delta_x_vector_corrected, label_mtrx)"
      ],
      "metadata": {
        "id": "Jya2oZggTFih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting into training and testing sets:"
      ],
      "metadata": {
        "id": "qR7bz48eTaGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.8, \n",
        "                                                    random_state=4)"
      ],
      "metadata": {
        "id": "r66Q2HMcTfA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train/test the model:"
      ],
      "metadata": {
        "id": "REScaiO8Upxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred_train, Y_pred_test = train_test('chains', X_train, X_test)"
      ],
      "metadata": {
        "id": "j8scTpHzUrwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model's performance evaluation:"
      ],
      "metadata": {
        "id": "lmJzIahZU9cc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "micro_f1 = f1_score(Y_train, Y_pred_train>0.5, average='micro')\n",
        "                     \n",
        "macro_f1 = f1_score(Y_train, Y_pred_train>0.5, average='macro') \n",
        "jaccard_score_ = jaccard_score(Y_train, Y_pred_train >= .5, average='samples')\n",
        "error_training= 1-jaccard_score_\n",
        "\n",
        "print(\"training performance\")\n",
        "print(\"micro-F1: %s\"%micro_f1)\n",
        "print(\"macro-F1: %s\"%macro_f1)\n",
        "print(\"training error: %s\"%error_training)\n",
        "print(\"\")\n",
        "\n",
        "\"\"\" ******************************* test on test set \"\"\"\n",
        "\n",
        "\"\"\"  evauation \"\"\"\n",
        "\n",
        " \n",
        "micro_f1 = f1_score(Y_test, Y_pred_test>0.5, average='micro')\n",
        "                \n",
        "macro_f1 = f1_score(Y_test, Y_pred_test>0.5, average='macro') \n",
        "jaccard_score_ = jaccard_score(Y_test, Y_pred_test >= .5, average='samples')\n",
        "error_testng= 1-jaccard_score_\n",
        "performance_test=np.mean([micro_f1,macro_f1])\n",
        "print(\"test performance\")\n",
        "print(\"micro-F1: %s\"%micro_f1)\n",
        "print(\"macro-F1: %s\"%macro_f1)\n",
        "print(\"Performance: %s\"%performance_test)\n",
        "print(\"testing error: %s\"%error_testng)\n",
        "print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIMsqAOcU80s",
        "outputId": "43d84004-b4b9-49af-ccef-341efe2902bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training performance\n",
            "micro-F1: 0.9994722955145119\n",
            "macro-F1: 0.9989572471324296\n",
            "training error: 0.06894484412470026\n",
            "\n",
            "test performance\n",
            "micro-F1: 0.8739384288747346\n",
            "macro-F1: 0.7160122097762324\n",
            "Performance: 0.7949753193254835\n",
            "testing error: 0.2082915810208975\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confusion matrix:"
      ],
      "metadata": {
        "id": "fCepK9qaXl9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "conf_mtrx_train =multilabel_confusion_matrix(Y_train, Y_pred_train>0.5)\n",
        "conf_mtrx_test =multilabel_confusion_matrix(Y_test, Y_pred_test>0.5)\n",
        "\n",
        "dic ={}\n",
        "keys = range(len(targetAppsNames))\n",
        "values = targetAppsNames\n",
        "for i in keys:\n",
        "    dic[i] = values[i]\n",
        "\n",
        "def f_score_train(column):\n",
        "\n",
        "    # tn = conf_mtrx_train[column, 0, 0]\n",
        "    tp = conf_mtrx_train[column, 1, 1]\n",
        "    fn = conf_mtrx_train[column, 1, 0]\n",
        "    fp = conf_mtrx_train[column, 0, 1]\n",
        "    f1 = (2*tp)/((2*tp)+fp+fn)\n",
        "    return f1\n",
        "\n",
        "def f_score_test(column):\n",
        "\n",
        "    # tn = conf_mtrx_test[column, 0, 0]\n",
        "    tp = conf_mtrx_test[column, 1, 1]\n",
        "    fn = conf_mtrx_test[column, 1, 0]\n",
        "    fp = conf_mtrx_test[column, 0, 1]\n",
        "    f1 = (2*tp)/((2*tp)+fp+fn)\n",
        "    return f1\n",
        "\n",
        "print(\"TRAIN\")\n",
        "for i in dic:\n",
        "    print (\"{0} F-score : {1}\".format(dic[i],f_score_train(int(i))))\n",
        "print(\"\")\n",
        "print(\"TEST\")\n",
        "for i in dic:\n",
        "    print (\"{0} F-score : {1}\".format(dic[i],f_score_test(int(i))))\n",
        "print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypHO1rrYXnw3",
        "outputId": "5439aa5e-dee6-4da7-cc00-158fae4d95bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN\n",
            "oven F-score : 1.0\n",
            "fridge F-score : 1.0\n",
            "dish.W F-score : 1.0\n",
            "lights F-score : 1.0\n",
            "washing.M F-score : 1.0\n",
            "mic F-score : 0.9927007299270073\n",
            "stove F-score : 1.0\n",
            "\n",
            "TEST\n",
            "oven F-score : 0.15053763440860216\n",
            "fridge F-score : 0.8615847542627884\n",
            "dish.W F-score : 0.5836298932384342\n",
            "lights F-score : 0.9581514762516046\n",
            "washing.M F-score : 0.9262295081967213\n",
            "mic F-score : 0.737527114967462\n",
            "stove F-score : 0.794425087108014\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of unique combinations of the different loads:"
      ],
      "metadata": {
        "id": "KoDjoLGtX6jC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#unique_rows_total = np.unique(Y, axis=0)\n",
        "unique_rows_train = np.unique(Y_train, axis=0)\n",
        "unique_rows_test = np.unique(Y_test, axis=0)\n",
        "#print(\"Number of unique combinantions in total: {}\".format(len(unique_rows_total)))\n",
        "print(\"Number of unique combinantions in training: {}\".format(len(unique_rows_train)))\n",
        "print(\"Number of unique combinantions in testing: {}\".format(len(unique_rows_test)))\n",
        "print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0QAdSdaX_ZH",
        "outputId": "e3db0644-8d01-4467-b051-022ca7e6bd64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique combinantions in training: 29\n",
            "Number of unique combinantions in testing: 43\n",
            "\n"
          ]
        }
      ]
    }
  ]
}